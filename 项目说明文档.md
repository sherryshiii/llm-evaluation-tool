# 大模型评测工具项目文档

## 项目概述

### 项目名称
大模型评测工具 (LLM Evaluation Tool)

### 项目目标
本项目旨在构建一个全面的大语言模型评测平台，用于：
1. **单条对比评测**：对单个问题在不同模型间进行横向对比评测
2. **批量评测**：基于Excel/CSV数据进行大规模批量评测
3. **多维度评估**：支持文本、JSON、地区信息等多种比较方式
4. **性能分析**：提供详细的响应时间、Token消耗、准确率等统计信息

### 业务价值
- 帮助研发团队快速评估不同LLM模型的表现
- 为模型选型提供数据支撑
- 提高模型测试效率，降低人工成本
- 支持大规模自动化测试场景

## 项目架构

### 技术栈
- **前端框架**：Gradio (Python Web UI框架)
- **后端语言**：Python 3.8+
- **数据处理**：Pandas, OpenPyXL
- **并发处理**：AsyncIO, AioHTTP
- **模型接口**：OpenAI API兼容接口
- **配置管理**：YAML
- **日志系统**：Python Logging + Watchdog

### 核心组件

#### 1. 主程序模块 (main.py)
- 应用程序入口
- Gradio界面初始化
- 多标签页管理
- 实时日志系统

#### 2. 配置管理模块 (config.py)
- LLM平台配置管理
- 模型信息获取和筛选
- 并发限制配置
- 价格策略管理

#### 3. LLM客户端模块 (llm_client.py)
- 多平台API适配
- 并发请求管理
- 错误处理和重试机制
- 响应时间统计

#### 4. 界面模块
- **ui_single.py**：单条对比界面
- **ui_table.py**：批量评测界面
- **ui_shared.py**：共享UI组件

#### 5. 工具模块 (utils/)
- **comparator.py**：多种比较算法实现
- **resp_parser.py**：响应内容解析器
- **log.py**：日志工具

## 功能特性

### 1. 单条对比评测
- 支持多模型同时对比
- 实时响应时间显示
- Token消耗统计
- 自定义系统提示词和用户提示词

### 2. 批量评测
- Excel/CSV文件批量导入
- 模板变量替换 (如 $input)
- 并发处理提升效率
- 自动生成统计报表

### 3. 多种比较方式
- **字符串比较**：基础文本匹配
- **JSON比较**：结构化数据对比
- **地区比较**：省市区地址信息匹配
- **复杂JSON比较**：多层级数据结构比较

### 4. 响应解析器
- 支持多种格式解析
- JSON修复和清理
- 错误处理机制

### 5. 模型管理
- 多平台支持（火山引擎、OpenAI等）
- 按价格筛选模型
- 并发限制配置
- 模型性能统计

## 技术方案

### 1. 异步并发架构
```python
# 使用AsyncIO实现高并发请求
async def multi_request(tasks, progress):
    # 按平台分组处理并发限制
    # 使用信号量控制并发数
    # 实时更新进度条
```

### 2. 模块化设计
- 界面与业务逻辑分离
- 可插拔的比较算法
- 配置驱动的模型管理

### 3. 数据流处理
```
Excel/CSV → 数据验证 → 模板替换 → 并发请求 → 响应解析 → 结果比较 → 统计分析 → 导出结果
```

### 4. 错误处理策略
- 多层次异常捕获
- 优雅降级处理
- 详细错误日志记录

## 部署方案

### 1. 本地开发环境
```bash
# 安装依赖
pip install -r requirements.txt

# 配置LLM API密钥
vim llm.yaml

# 启动服务
python main.py
```

### 2. Docker部署
```bash
# 构建镜像
docker build -t llm-eval .

# 运行容器
docker run -p 7860:7860 llm-eval
```

### 3. 生产环境建议
- 使用Nginx反向代理
- 配置SSL证书
- 设置环境变量管理敏感信息
- 日志轮转和监控告警

## 开发指南

### 1. 代码规范
- 代码紧凑，每行不超过120字符
- 注释使用中文，保持简洁
- 遵循奥卡姆剃刀原则，避免过度设计
- 及时清理无用代码和变量

### 2. 新增模型支持
1. 在 `llm.yaml` 中添加平台配置
2. 在 `llm_client.py` 中实现对应的API适配
3. 更新 `config.py` 中的模型获取逻辑

### 3. 新增比较算法
1. 在 `utils/comparator.py` 中实现新函数
2. 在 `COMPARE_FUNCTIONS` 字典中注册
3. 在界面中添加选项

### 4. 扩展响应解析器
1. 在 `utils/resp_parser.py` 中添加新解析逻辑
2. 在界面中提供选择选项

## 数据格式说明

### 1. Excel/CSV格式要求
- 必须包含提示词中使用的变量列（如 `$input` 对应 `input` 列）
- 可选包含 `期望答案` 列用于准确率计算
- 支持多个变量替换（如 `$question`, `$context` 等）

### 2. 配置文件格式 (llm.yaml)
```yaml
platforms:
  platform_name:
    name: "显示名称"
    url: "API端点"
    api_key: "API密钥"
    concurrent: 并发数限制
    models:
      - showname: "模型显示名"
        model: "模型标识"
        price: "价格等级"
```

## 性能优化

### 1. 并发优化
- 按平台设置不同并发限制
- 使用连接池复用HTTP连接
- 实现请求队列管理

### 2. 内存优化
- 分批处理大文件
- 及时释放不用的DataFrame
- 使用生成器处理大数据集

### 3. 用户体验优化
- 实时进度反馈
- 错误信息友好提示
- 支持中断和重试

## 常见问题

### 1. API配置问题
- 检查 `llm.yaml` 中的API密钥是否正确
- 确认网络连接和代理设置
- 验证模型名称是否匹配

### 2. 文件格式问题
- 确保Excel/CSV文件编码正确
- 检查列名是否与变量对应
- 验证数据格式是否符合要求

### 3. 性能问题
- 调整并发数设置
- 检查网络延迟
- 监控内存使用情况

## 后续扩展计划

### 1. 功能扩展
- 支持更多模型平台
- 增加图像和语音评测
- 实现自定义评分算法
- 添加A/B测试功能

### 2. 技术优化
- 引入分布式任务队列
- 实现结果缓存机制
- 添加更详细的监控指标
- 优化大文件处理性能

### 3. 用户体验
- 实现用户管理系统
- 添加测试历史记录
- 支持测试模板保存
- 增加数据可视化图表

---

